{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0-rc1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"../data/data_dir/\"\n",
    "images = os.listdir(os.path.join(base,\"Images/\"))\n",
    "masks = os.listdir(os.path.join(base, \"Masks/\"))\n",
    "images.sort()\n",
    "masks.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_example(image_string, label_string):\n",
    "    image_shape = tf.image.decode_png(image_string).shape\n",
    "\n",
    "    feature = {\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_string])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"png\"])),\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_shape[0]])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_shape[1]])),\n",
    "        'image/segmentation/class/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[label_string])),\n",
    "        'image/segmentation/class/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"png\"])),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48M\ttrainseg.tfrecords\n"
     ]
    }
   ],
   "source": [
    "record_file = 'trainseg.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for image, mask in zip(images, masks):\n",
    "        img = open(os.path.join(base,\"Images/\",image), 'rb').read()\n",
    "        msk = open(os.path.join(base,\"Masks/\",mask), 'rb').read()\n",
    "        example = image_example(img, msk)\n",
    "        writer.write(example.SerializeToString())\n",
    "!du -sh {record_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-299-719c169bd973>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-299-719c169bd973>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \"\"\"Function to parse the example proto.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def _parse_function(example_proto):\n",
    "    \"\"\"Function to parse the example proto.\n",
    "\n",
    "    Args:\n",
    "      example_proto: Proto in the format of tf.Example.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary with parsed image, label, height, width and image name.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: Label is of wrong shape.\n",
    "    \"\"\"\n",
    "\n",
    "    # Currently only supports jpeg and png.\n",
    "    # Need to use this logic because the shape is not known for\n",
    "    # tf.image.decode_image and we rely on this info to\n",
    "    # extend label if necessary.\n",
    "    def _decode_image(content, channels):\n",
    "      return tf.cond(\n",
    "          tf.image.is_jpeg(content),\n",
    "          lambda: tf.image.decode_jpeg(content, channels),\n",
    "          lambda: tf.image.decode_png(content, channels))\n",
    "\n",
    "    features = {\n",
    "        'image/encoded':\n",
    "            tf.io.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/filename':\n",
    "            tf.io.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format':\n",
    "            tf.io.FixedLenFeature((), tf.string, default_value='png'),\n",
    "        'image/height':\n",
    "            tf.io.FixedLenFeature((), tf.int64, default_value=0),\n",
    "        'image/width':\n",
    "            tf.io.FixedLenFeature((), tf.int64, default_value=0),\n",
    "        'image/segmentation/class/encoded':\n",
    "            tf.io.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/segmentation/class/format':\n",
    "            tf.io.FixedLenFeature((), tf.string, default_value='png'),\n",
    "    }\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, features)\n",
    "\n",
    "    image = _decode_image(parsed_features['image/encoded'], channels=3)\n",
    "    print(image.shape)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image_dataset = tf.data.TFRecordDataset('../data/trainseg.tfrecords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 3)\n"
     ]
    }
   ],
   "source": [
    "it = iter(raw_image_dataset.map(_parse_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(240, 320, 3), dtype=uint8, numpy=\n",
       "array([[[147, 178, 180],\n",
       "        [147, 178, 180],\n",
       "        [147, 178, 180],\n",
       "        ...,\n",
       "        [ 60,  80,  87],\n",
       "        [ 62,  82,  89],\n",
       "        [ 63,  83,  90]],\n",
       "\n",
       "       [[169, 199, 201],\n",
       "        [169, 199, 201],\n",
       "        [169, 199, 201],\n",
       "        ...,\n",
       "        [ 76,  96, 103],\n",
       "        [ 78,  98, 105],\n",
       "        [ 78,  98, 105]],\n",
       "\n",
       "       [[186, 214, 217],\n",
       "        [186, 214, 217],\n",
       "        [186, 214, 217],\n",
       "        ...,\n",
       "        [ 89, 109, 116],\n",
       "        [ 90, 110, 117],\n",
       "        [ 90, 110, 117]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 58,  57,  62],\n",
       "        [ 58,  57,  62],\n",
       "        [ 57,  56,  61],\n",
       "        ...,\n",
       "        [ 36,  36,  36],\n",
       "        [ 36,  36,  36],\n",
       "        [ 37,  37,  37]],\n",
       "\n",
       "       [[ 60,  59,  64],\n",
       "        [ 59,  58,  63],\n",
       "        [ 58,  57,  62],\n",
       "        ...,\n",
       "        [ 38,  38,  38],\n",
       "        [ 38,  38,  38],\n",
       "        [ 38,  38,  38]],\n",
       "\n",
       "       [[ 60,  59,  64],\n",
       "        [ 60,  59,  64],\n",
       "        [ 59,  58,  63],\n",
       "        ...,\n",
       "        [ 39,  39,  39],\n",
       "        [ 39,  39,  39],\n",
       "        [ 39,  39,  39]]], dtype=uint8)>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHONPATH=.:./slim python3.7 deeplab/train.py \\\n",
    "    --logtostderr \\\n",
    "    --training_number_of_steps=30000 \\\n",
    "    --train_split=\"train\" \\\n",
    "    --model_variant=\"xception_65\" \\\n",
    "    --atrous_rates=6 \\\n",
    "    --atrous_rates=12 \\\n",
    "    --atrous_rates=18 \\\n",
    "    --output_stride=16 \\\n",
    "    --decoder_output_stride=4 \\\n",
    "    --train_crop_size=513 \\\n",
    "    --train_crop_size=513 \\\n",
    "    --train_batch_size=1 \\\n",
    "    --dataset=\"trainseg\" \\\n",
    "    --tf_initial_checkpoint=${PATH_TO_INITIAL_CHECKPOINT} \\\n",
    "    --train_logdir=/home/ubuntuadm/trainseg/train_log \\\n",
    "    --dataset_dir=/home/ubuntuadm/trainseg/data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
